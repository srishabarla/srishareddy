{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishabarla/srishareddy/blob/main/day3_list_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KZ91_NegViLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## problem 1\n",
        "Create a dictionary where the keys correspond to the area of a flat adn value for each is the price of the house for 10 flat in kompally"
      ],
      "metadata": {
        "id": "B1Fhdmi9V3LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of house area and house prices of 10 houses in\n",
        "flat_prices_in_kompally = {\n",
        "    'Area1': 1000000,\n",
        "    'Area2': 1200000,\n",
        "    'Area3': 950000,\n",
        "    'Area4': 1500000,\n",
        "    'Area5': 800000,\n",
        "    'Area6': 1100000,\n",
        "    'Area7': 1300000,\n",
        "    'Area8': 900000,\n",
        "    'Area9': 1100000,\n",
        "    'Area10': 1400000\n",
        "}\n",
        "for area, price in flat_prices_in_kompally.items():\n",
        "    print(f\"Price for {area}: ${price}\")\n",
        "area3_price = flat_prices_in_kompally.get('Area3')\n",
        "print(f\"Price for Area3: ${area3_price}\")"
      ],
      "metadata": {
        "id": "zihWmw3NWXQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3dc246-12d5-42e0-9c57-8c30573f3acc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price for Area1: $1000000\n",
            "Price for Area2: $1200000\n",
            "Price for Area3: $950000\n",
            "Price for Area4: $1500000\n",
            "Price for Area5: $800000\n",
            "Price for Area6: $1100000\n",
            "Price for Area7: $1300000\n",
            "Price for Area8: $900000\n",
            "Price for Area9: $1100000\n",
            "Price for Area10: $1400000\n",
            "Price for Area3: $950000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now broker predicts the price as can be written as : <b><code>predicted_price(area) = 0.06*area + 15</code></b>"
      ],
      "metadata": {
        "id": "JI2jaDf8Wp2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the concept of list comprehension,create a list of predicted flat price using the above formula\n"
      ],
      "metadata": {
        "id": "v_K_Z8dtW6-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predicted_price(area):\n",
        "    return 0.06 * area + 15\n",
        "areas = [100, 120, 80, 150, 90, 110, 130, 95, 105, 140]\n",
        "predicted_prices = [predicted_price(area) for area in areas]\n",
        "for area, price in zip(areas, predicted_prices):\n",
        "    print(f\"Predicted price for Area {area}: ${price}\")\n"
      ],
      "metadata": {
        "id": "F732P6DHXVYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76255aa2-f806-47aa-992a-f898dc57ce2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted price for Area 100: $21.0\n",
            "Predicted price for Area 120: $22.2\n",
            "Predicted price for Area 80: $19.8\n",
            "Predicted price for Area 150: $24.0\n",
            "Predicted price for Area 90: $20.4\n",
            "Predicted price for Area 110: $21.6\n",
            "Predicted price for Area 130: $22.8\n",
            "Predicted price for Area 95: $20.7\n",
            "Predicted price for Area 105: $21.3\n",
            "Predicted price for Area 140: $23.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create another list which contains the squares of differences of each element of the two price lists : the actual prices and the predicted prices\n"
      ],
      "metadata": {
        "id": "-ZrRO78DXZq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_prices = [1000000, 1200000, 950000, 1500000, 800000, 1100000, 1300000, 900000, 1100000, 1400000]\n",
        "squares_of_differences = [(actual - predicted)**2 for actual, predicted in zip(actual_prices, predicted_prices)]\n",
        "for area, actual, predicted, square_diff in zip(areas, actual_prices, predicted_prices, squares_of_differences):\n",
        "    print(f\"Area {area}: Actual Price=${actual}, Predicted Price=${predicted}, Square of Difference=${square_diff}\")"
      ],
      "metadata": {
        "id": "dEpxZqK7XzeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b06270-4751-4a80-dde4-c49fd39dd4ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area 100: Actual Price=$1000000, Predicted Price=$21.0, Square of Difference=$999958000441.0\n",
            "Area 120: Actual Price=$1200000, Predicted Price=$22.2, Square of Difference=$1439946720492.84\n",
            "Area 80: Actual Price=$950000, Predicted Price=$19.8, Square of Difference=$902462380392.0399\n",
            "Area 150: Actual Price=$1500000, Predicted Price=$24.0, Square of Difference=$2249928000576.0\n",
            "Area 90: Actual Price=$800000, Predicted Price=$20.4, Square of Difference=$639967360416.1599\n",
            "Area 110: Actual Price=$1100000, Predicted Price=$21.6, Square of Difference=$1209952480466.5598\n",
            "Area 130: Actual Price=$1300000, Predicted Price=$22.8, Square of Difference=$1689940720519.8398\n",
            "Area 95: Actual Price=$900000, Predicted Price=$20.7, Square of Difference=$809962740428.4901\n",
            "Area 105: Actual Price=$1100000, Predicted Price=$21.3, Square of Difference=$1209953140453.69\n",
            "Area 140: Actual Price=$1400000, Predicted Price=$23.4, Square of Difference=$1959934480547.5603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <u> Problem 2</u>\n",
        "\n",
        "## A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula:\n",
        "\n",
        "$$ h(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} }  $$\n",
        "\n",
        "\n",
        "## Range is defined as all the possible values which the function $h(x)$ can take. Domain is defined as all the possible values which $x$ can take. In this case, range of the function is between 0 to 1 and the domain of the function is all real numbers"
      ],
      "metadata": {
        "id": "PEC_OtXRXrCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You are given a list of values of $x$. You need to use list comprehension to calculate the corresponding transformation according to the sigmoid function defined above"
      ],
      "metadata": {
        "id": "YZCTres_XsB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "x_values = [0, 1, 2, 3, 4]\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "sigmoid_values = [sigmoid(x) for x in x_values]\n",
        "for x, sigmoid_value in zip(x_values, sigmoid_values):\n",
        "    print(f\"Sigmoid transformation for x={x}: {sigmoid_value}\")"
      ],
      "metadata": {
        "id": "TmYb4SluYC4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e00815-2731-4eed-a285-7b0d40daf971"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid transformation for x=0: 0.5\n",
            "Sigmoid transformation for x=1: 0.7310585786300049\n",
            "Sigmoid transformation for x=2: 0.8807970779778823\n",
            "Sigmoid transformation for x=3: 0.9525741268224334\n",
            "Sigmoid transformation for x=4: 0.9820137900379085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u> Problem 3</u>\n",
        "\n",
        "## You are given a sentence : <code>\"I have been walking and running and dancing and smiling and laughing all my life, yet it all seems pointless. So i stoped thinking and started doing\"</code>\n",
        "\n",
        "## You are required to extract all those words from this sentence in a list which ends with <code>ing</code>"
      ],
      "metadata": {
        "id": "j31JnVUtYNU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I have been walking and running and dancing and smiling and laughing all my life, yet it all seems pointless. So I stopped thinking and started doing\"\n",
        "ing_words = [word.strip(\",.\") for word in sentence.split() if word.endswith(\"ing\")]\n",
        "print(\"Words ending with 'ing':\", ing_words)\n"
      ],
      "metadata": {
        "id": "8s6rFnH2YO1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31590ce8-5292-4490-8b34-9b602d55614e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words ending with 'ing': ['walking', 'running', 'dancing', 'smiling', 'laughing', 'thinking', 'doing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <u> Problem 4</u>\n",
        "\n",
        "#### Natural Language Processing or NLP is one of the most promising fields in Machine Learning. Most of the times in NLP we deal with the textual data (a bunch of strings). Sometimes when we are processing the text, it is a common practice to get rid of some set of stop words from our original text. By default stop words are very common words used in English language such as and, or, punctuations etc.\n",
        "\n",
        "#### In this exercise, you are provided with a default set of stop words and you need to add some extra set of custom words and remove these words from the given sentence and obtain the sentence without the stop words"
      ],
      "metadata": {
        "id": "rYwInhGlYOyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default set of stop words\n",
        "stop_words = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
        "              \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
        "              \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n",
        "              \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
        "              \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n",
        "              \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n",
        "              \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
        "              \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
        "              \"don\", \"should\", \"now\"}"
      ],
      "metadata": {
        "id": "RbyJscXWYh0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "              \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "              \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
        "              \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
        "              \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\",\n",
        "              \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
        "              \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n",
        "              \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n",
        "              \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
        "              \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
        "              \"don\", \"should\", \"now\"}\n",
        "def remove_stopwords(sentence, custom_stopwords=[]):\n",
        "    default_stopwords = set(stopwords.words('same'))\n",
        "    stop_words = default_stopwords.union(set(custom_stopwords))\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    result_sentence = ' '.join(filtered_words)\n",
        "    return result_sentence\n",
        "input_sentence = \"Natural Language Processing or NLP is one of the most promising fields in Machine Learning. Most of the times in NLP we deal with the textual data (a bunch of strings). Sometimes when we are processing the text, it is a common practice to get rid of some set of stop words from our original text. By default stop words are very common words used in English language such as and, or, punctuations etc.\"\n",
        "custom_stopwords = [\"hello\",\"folks\",\"good\",\"morning\",\"half\",\"year\"]\n",
        "result_sentence = remove_stopwords(input_sentence, custom_stopwords)\n",
        "print(\"Original Sentence:\")\n",
        "print(input_sentence)\n",
        "print(\"\\nSentence without Stop Words:\")\n",
        "print(result_sentence)\n"
      ],
      "metadata": {
        "id": "RzO-jDZVY6wD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "b902d7a4-579d-4f79-88c1-d4c12fc28b86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'set' object has no attribute 'words'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9980cdc53c02>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Natural Language Processing or NLP is one of the most promising fields in Machine Learning. Most of the times in NLP we deal with the textual data (a bunch of strings). Sometimes when we are processing the text, it is a common practice to get rid of some set of stop words from our original text. By default stop words are very common words used in English language such as and, or, punctuations etc.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mcustom_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"folks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"morning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"half\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mresult_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original Sentence:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9980cdc53c02>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(sentence, custom_stopwords)\u001b[0m\n\u001b[1;32m     13\u001b[0m               \"don\", \"should\", \"now\"}\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdefault_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'words'"
          ]
        }
      ]
    }
  ]
}